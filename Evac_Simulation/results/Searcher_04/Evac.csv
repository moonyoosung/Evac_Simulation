Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
10000,1.5882399,317.14285714285717,-1.1607652,-75.56607283705047,-75.56607283705047,3.758815,0.097407155,0.0002983692,0.29891282,0.004972874,1.0
20000,0.78997475,373.76666666666665,-0.7913226,-15.033016850923499,-15.033016850923499,0.33830106,0.09138086,0.00029553528,0.29702353,0.0049257367,1.0
30000,0.36433125,384.8181818181818,-0.10145668,-1.7820454917170785,-1.7820454917170785,0.045110602,0.09181684,0.0002924657,0.29497713,0.0048746793,1.0
40000,0.24637935,399.0,-0.05660668,-1.092517305037071,-1.092517305037071,0.0010703871,0.10236137,0.00028943003,0.29295334,0.0048241857,1.0
50000,0.18330483,399.0,-0.050899573,-1.0192000672221184,-1.0192000672221184,0.00039951084,0.09548858,0.00028654363,0.29102907,0.004776175,1.0
