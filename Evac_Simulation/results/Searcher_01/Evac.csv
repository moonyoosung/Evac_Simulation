Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,0.9882458,294.4573170731707,-0.1875425,-1.0244798634958878,-1.0244798634958878,1.0594792,0.09859183,0.0002847706,0.28984705,0.0047466834,1.0
200000,0.6748865,291.19767441860466,0.30798298,0.5070932104988674,0.5070932104988674,0.89413065,0.096959285,0.00025492525,0.26995018,0.004250256,1.0
300000,0.58501136,287.34971098265896,-0.24107134,-0.9131750861214923,-0.9131750861214923,0.7266016,0.0990605,0.00022492393,0.24994928,0.0037512335,1.0
400000,0.5587519,302.41818181818184,-0.23368123,-1.2703028308566322,-1.2703028308566322,0.65492785,0.09610975,0.00019509855,0.23006564,0.003255138,1.0
500000,0.44990203,325.6221498371336,-0.58187234,-2.5225567615618294,-2.5225567615618294,0.40702924,0.09632034,0.00016508694,0.21005788,0.0027559446,1.0
600000,0.39615503,305.56307692307695,-0.38826844,-1.0989536177510253,-1.0989536177510253,0.54722923,0.09548164,0.00013507031,0.19004686,0.0022566684,1.0
700000,0.2592204,295.92011834319527,-0.06075996,-0.28386071953404496,-0.28386071953404496,0.71977633,0.096046254,0.0001051297,0.17008643,0.0017586561,1.0
800000,0.20884177,270.44054054054055,0.5638399,1.2512974620998107,1.2512974620998107,0.909126,0.09353169,7.502584e-05,0.15001717,0.0012579283,1.0
900000,0.18694258,253.06666666666666,0.7292831,1.8679307680648864,1.8679307680648864,1.0523934,0.09696235,4.494089e-05,0.12996055,0.0007575154,1.0
1000000,0.18011819,257.0671834625323,0.771192,1.3997431073414406,1.3997431073414406,1.0939541,0.09399319,1.4950738e-05,0.10996711,0.00025867895,1.0
