Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.7625401,332.88028169014086,-0.836374,-39.35511711035186,-39.35511711035186,3.7369277,0.022904994,0.00028437452,0.19479153,0.004740096,1.0
100000,1.5408157,354.468085106383,-2.4732382,-27.079711356333323,-27.079711356333323,1.494945,0.022221487,0.00025652332,0.18550774,0.004276837,1.0
150000,1.2476718,367.4779411764706,-3.3676245,-16.83446786166641,-16.83446786166641,0.87655866,0.023585513,0.00022571502,0.17523833,0.0037643916,1.0
200000,0.8672207,364.43703703703704,-3.3161833,-9.290404128366047,-9.290404128366047,0.44371834,0.026024187,0.0001948545,0.16495147,0.0032510788,1.0
250000,0.58588135,371.609022556391,-2.80442,-5.569620722862787,-5.569620722862787,0.2852214,0.02200138,0.00016398102,0.15466031,0.0027375496,1.0
300000,0.37295577,364.72463768115944,-2.1334507,-4.152384468178818,-4.152384468178818,0.29856917,0.02291832,0.00013313635,0.14437875,0.0022245,1.0
350000,0.27232248,367.41911764705884,-1.5431082,-4.346309169770821,-4.346309169770821,0.35365653,0.019543726,0.00010535648,0.1351188,0.001762428,1.0
400000,0.17917743,375.3984962406015,-1.1325372,-3.2706620281744274,-3.2706620281744274,0.10157366,0.021581491,7.755753e-05,0.12585248,0.0013000389,1.0
450000,0.10979156,375.1526717557252,-0.8626585,-2.4279087510045247,-2.4279087510045247,0.11524427,0.021034474,4.6614565e-05,0.11553816,0.0007853542,1.0
500000,0.093092926,364.53284671532845,-0.73004675,-2.832569669633016,-2.832569669633016,0.1377683,0.025457349,1.5716612e-05,0.10523883,0.00027141807,1.0
